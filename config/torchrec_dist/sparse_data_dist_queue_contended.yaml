# Queue contention config to study GPU_HW_MAX_QUEUES paradox behaviour.
RunOptions:
  world_size: 2
  num_batches: 12
  sharding_type: table_column_wise
  compute_kernel: fused
  profile_dir: "./trace_queue_contended"
  name: "sparse_data_dist_queue_contended"

PipelineConfig:
  pipeline: "sparse"

ModelSelectionConfig:
  model_name: "test_sparse_nn"
  batch_size: 40960
  num_float_features: 32
  feature_pooling_avg: 8
  pin_memory: true
  long_kjt_indices: true
  long_kjt_offsets: true
  long_kjt_lengths: true

EmbeddingTablesConfig:
  num_unweighted_features: 256
  num_weighted_features: 64
  embedding_feature_dim: 128
  additional_tables:
    - - name: cw_hot_table_0
        embedding_dim: 512
        num_embeddings: 1_200_000
        feature_names: ["cw_hot_0"]
      - name: cw_hot_table_1
        embedding_dim: 512
        num_embeddings: 900_000
        feature_names: ["cw_hot_1"]
    - - name: weighted_cw_table_0
        embedding_dim: 256
        num_embeddings: 800_000
        feature_names: ["weighted_cw_0"]
      - name: weighted_cw_table_1
        embedding_dim: 256
        num_embeddings: 800_000
        feature_names: ["weighted_cw_1"]
