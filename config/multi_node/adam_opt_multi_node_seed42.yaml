# AdamW optimizer variant of the user workload.
# AdamW is more numerically stable than Shampoo for large embeddings.
# SEED: 42 (baseline)

logging:
  level: DEBUG

training:
  epochs: 10
  batch_size: 128
  gradient_accumulation: 1
  mixed_precision: bf16
  max_steps: 4400
  grad_clip_norm: 1
  output_dir: artifacts/user_adamw_seed42
  log_interval: 1
  additional_compute_streams: 2
  lightweight_op_waves: 3
  inject_allreduce_copies: false
  allreduce_stress_level: 8

optimizer:
  name: adamw
  lr: 0.0002
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1.0e-12

scheduler:
  warmup_steps: 200
  total_steps: 2200

dataset:
  num_samples: 200000
  sequence_length: 512
  dense_dim: 256
  sparse_features: 64
  vocab_size: 350000
  num_dense_features: 32
  seed: 42

model:
  vocab_size: 350000
  embedding_dim: 256
  num_dense_features: 32
  dense_dim: 256
  model_dim: 1024
  num_heads: 16
  num_layers: 18
  dropout: 0.1
  mlp_hidden_dim: 4096

fsdp:
  sharding_strategy: hybrid_shard
  backward_prefetch: BACKWARD_PRE
  use_orig_params: true
  limit_all_gathers: true
  forward_prefetch: true
  sync_module_states: true
  param_init_device: meta

distributed:
  backend: nccl
  mode: fsdp
  bucket_cap_mb: 128
  gradient_as_bucket_view: true
  static_graph: true
  find_unused_parameters: false

compile:
  enabled: false
  backend: inductor
  mode: max-autotune
  fullgraph: false
  dynamic: false

streams:
  num_streams: 4
  high_priority:
    - allreduce
    - reducescatter
  stream_assignments:
    compute:
      - dev6_stream3
      - dev6_stream9
    communication:
      - dev6_stream13
      - dev6_stream17
    reducescatter:
      - dev6_stream22
    aux:
      - dev6_stream0

dataloader:
  num_workers: 0
  pin_memory: true

profiling:
  enabled: false
  wait: 0
  warmup: 0
  active: 2000
  repeat: 1
  record_shapes: false
  profile_memory: false
  with_stack: false
  with_flops: false
  tensorboard: false
  chrome_trace: true
  trace_filename: user_adamw.json

tracelens:
  enabled: false

