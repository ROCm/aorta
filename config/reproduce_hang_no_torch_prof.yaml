# Configuration to reproduce hang - PyTorch profiler disabled
# Custom StreamProfiler still generates rank_*_metrics.jsonl files

logging:
  level: INFO

training:
  epochs: 1
  batch_size: 4
  gradient_accumulation: 1
  mixed_precision: fp16
  grad_clip_norm: 1.0
  log_interval: 5
  max_steps: 100
  output_dir: artifacts_hang_repro

optimizer:
  lr: 0.0003
  weight_decay: 0.01
  betas: [0.9, 0.98]

scheduler:
  warmup_steps: 100
  total_steps: 2000

dataset:
  num_samples: 10000
  sequence_length: 64
  dense_dim: 128
  sparse_features: 32
  vocab_size: 60000
  num_dense_features: 16
  seed: 42

model:
  vocab_size: 60000
  embedding_dim: 192
  num_dense_features: 16
  dense_dim: 128
  model_dim: 768
  num_heads: 12
  num_layers: 8
  dropout: 0.1
  mlp_hidden_dim: 2048

fsdp:
  sharding_strategy: full_shard
  backward_prefetch: BACKWARD_PRE
  use_orig_params: false
  limit_all_gathers: false
  forward_prefetch: false
  sync_module_states: true
  param_init_device: cpu

compile:
  enabled: false
  backend: inductor
  mode: max-autotune
  fullgraph: false
  dynamic: false
  options: {}

dataloader:
  num_workers: 4
  pin_memory: true

profiling:
  enabled: false  # Disabled - PyTorch profiler crashes on ROCm with multi-stream RCCL
  wait: 1
  warmup: 1
  active: 2
  repeat: 1
  record_shapes: false
  profile_memory: false
  with_stack: false
  with_flops: false
  tensorboard: false
  chrome_trace: false
  trace_filename: trace.json
