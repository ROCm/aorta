{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AORTA FSDP2 Training Launcher\n",
    "\n",
    "Use this notebook to orchestrate AORTA's multi-stream FSDP2 workload from a Jupyter session. It wraps the existing CLI so you can launch short sanity runs, full-scale multi-GPU jobs, and cross-accelerator experiments directly from the notebook interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Run the setup cell to register the repository package path and inspect the active accelerator.\n",
    "2. Adjust the configuration path or overrides to match the experiment you want to execute.\n",
    "3. Invoke one of the helper functions:\n",
    "   - `run_training_single_process(...)` for quick smoke tests on a single GPU or CPU inside the notebook kernel.\n",
    "   - `run_training_torchrun(...)` to launch distributed jobs (NVIDIA/AMD) via `torchrun` while still streaming logs back into the notebook.\n",
    "4. Optionally analyse the produced artefacts (logs, JSONL traces) with the reporting utilities once a run completes.\n",
    "\n",
    "Each helper accepts the same CLI arguments as `train.py`, so you can reuse your existing YAML configs and override syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07dc471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /manrao/jsr_perf_max/aorta\n",
      "Python executable: /opt/conda/envs/py_3.10/bin/python\n",
      "Detected accelerator: amd | device_count=8 | primary_device=AMD Instinct MI350X\n"
     ]
    }
   ],
   "source": [
    "# --- Environment bootstrap -------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shlex\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:  # pragma: no cover - defensive fallback\n",
    "    torch = None\n",
    "\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    markers = [start] + list(start.parents)\n",
    "    for candidate in markers:\n",
    "        if (candidate / 'config' / 'default.yaml').exists():\n",
    "            return candidate\n",
    "        if (candidate / '.git').exists() and (candidate / 'src').exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "REPO_ROOT = _find_repo_root(Path.cwd().resolve())\n",
    "SRC_ROOT = REPO_ROOT / 'src'\n",
    "if SRC_ROOT.exists() and str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_ROOT))\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "if torch is None:\n",
    "    print(\"PyTorch import failed; install torch before running training.\")\n",
    "else:\n",
    "    accelerator = 'cpu'\n",
    "    device_name = 'CPU'\n",
    "    if torch.cuda.is_available():\n",
    "        accelerator = 'nvidia'\n",
    "        if getattr(torch.version, 'hip', None):\n",
    "            accelerator = 'amd'\n",
    "        try:\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "        except Exception:  # pragma: no cover - query best effort\n",
    "            device_name = 'Unknown GPU'\n",
    "        device_count = torch.cuda.device_count()\n",
    "    else:\n",
    "        device_count = 0\n",
    "    print(f\"Detected accelerator: {accelerator} | device_count={device_count} | primary_device={device_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538a7c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0a0+git0bf8d8e'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62588313",
   "metadata": {},
   "source": [
    "## Inspect / Edit Configuration\n",
    "\n",
    "The helpers expect a valid config file (default: `config/default.yaml`). Update the path or override dictionary in the next cell to customise hyperparameters, profiling options, or output directory for your run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333b93cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config: /manrao/jsr_perf_max/aorta/config/default.yaml\n",
      "Top-level config sections: logging, training, optimizer, scheduler, dataset, model, fsdp, compile, dataloader, profiling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = REPO_ROOT / \"config\" / \"default.yaml\"\n",
    "print(f\"Using config: {CONFIG_PATH}\")\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "    base_config = yaml.safe_load(handle)\n",
    "\n",
    "# Display the top-level keys for quick reference\n",
    "print(\"Top-level config sections:\", \", \".join(base_config.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea529d1",
   "metadata": {},
   "source": [
    "## Training Launch Helpers\n",
    "\n",
    "The functions below wrap `train.py` with robust environment setup and argument handling. They default to short runs (`max_steps=5`) to keep notebook executions lightweightâ€”update or remove the overrides as needed for full-scale experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f416767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Optional, Sequence, Union\n",
    "\n",
    "if 'SRC_ROOT' not in globals():\n",
    "    notebook_root = Path.cwd().resolve()\n",
    "    guessed_src = notebook_root / 'src'\n",
    "    SRC_ROOT = guessed_src if guessed_src.exists() else notebook_root\n",
    "if 'CONFIG_PATH' not in globals():\n",
    "    repo_root = Path.cwd().resolve()\n",
    "    default_config = repo_root / 'config' / 'default.yaml'\n",
    "    CONFIG_PATH = default_config if default_config.exists() else repo_root / 'config.yaml'\n",
    "\n",
    "\n",
    "def _normalise_overrides(overrides: Optional[Union[Mapping[str, Union[str, int, float, bool]], Sequence[str]]]) -> Sequence[str]:\n",
    "    \"\"\"Convert override inputs into CLI-friendly dotted assignments.\"\"\"\n",
    "\n",
    "    if overrides is None:\n",
    "        return []\n",
    "    if isinstance(overrides, Mapping):\n",
    "        items = []\n",
    "        for key, value in overrides.items():\n",
    "            if isinstance(value, bool):\n",
    "                val_str = 'true' if value else 'false'\n",
    "            else:\n",
    "                val_str = str(value)\n",
    "            items.append(f\"{key}={val_str}\")\n",
    "        return sorted(items)\n",
    "    return list(overrides)\n",
    "\n",
    "\n",
    "def _prepare_env(extra_env: Optional[Mapping[str, str]] = None) -> dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    pythonpath = env.get('PYTHONPATH', '')\n",
    "    parts = [str(SRC_ROOT)] + ([pythonpath] if pythonpath else [])\n",
    "    env['PYTHONPATH'] = os.pathsep.join(parts)\n",
    "    if extra_env:\n",
    "        env.update(extra_env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def run_training_single_process(\n",
    "    config_path: Union[str, Path] = CONFIG_PATH,\n",
    "    overrides: Optional[Union[Mapping[str, Union[str, int, float, bool]], Sequence[str]]] = None,\n",
    "    *,\n",
    "    enable_rocm_metrics: bool = False,\n",
    "    env: Optional[Mapping[str, str]] = None,\n",
    "):\n",
    "    \"\"\"Execute `train.py` inside the notebook kernel using rank/world size = 1.\"\"\"\n",
    "\n",
    "    override_args = _normalise_overrides(overrides)\n",
    "    cmd: list[str] = [sys.executable, '../train.py', '--config', str(config_path)]\n",
    "    for item in override_args:\n",
    "        cmd.extend(['--override', item])\n",
    "    if enable_rocm_metrics:\n",
    "        cmd.append('--enable-rocm-metrics')\n",
    "\n",
    "    base_env = {\n",
    "        'MASTER_ADDR': '127.0.0.1',\n",
    "        'MASTER_PORT': os.environ.get('MASTER_PORT', '29500'),\n",
    "        'RANK': '0',\n",
    "        'WORLD_SIZE': '1',\n",
    "        'LOCAL_RANK': '0',\n",
    "    }\n",
    "\n",
    "    complete_env = _prepare_env(base_env)\n",
    "    if env:\n",
    "        complete_env.update(env)\n",
    "\n",
    "    print('Launching single-process training: ' + ' '.join(shlex.quote(token) for token in cmd))\n",
    "    return subprocess.run(cmd, env=complete_env, check=False)\n",
    "\n",
    "\n",
    "def run_training_torchrun(\n",
    "    *,\n",
    "    num_processes: int,\n",
    "    config_path: Union[str, Path] = CONFIG_PATH,\n",
    "    overrides: Optional[Union[Mapping[str, Union[str, int, float, bool]], Sequence[str]]] = None,\n",
    "    enable_rocm_metrics: bool = False,\n",
    "    extra_torchrun_args: Optional[Sequence[str]] = None,\n",
    "    env: Optional[Mapping[str, str]] = None,\n",
    "):\n",
    "    \"\"\"Launch distributed training via torchrun while streaming stdout/stderr into the notebook.\"\"\"\n",
    "\n",
    "    override_args = _normalise_overrides(overrides)\n",
    "    cmd: list[str] = [\n",
    "        'torchrun',\n",
    "        '--standalone',\n",
    "        '--nproc_per_node',\n",
    "        str(num_processes),\n",
    "    ]\n",
    "    if extra_torchrun_args:\n",
    "        cmd.extend(list(extra_torchrun_args))\n",
    "    cmd.extend(['train.py', '--config', str(config_path)])\n",
    "    for item in override_args:\n",
    "        cmd.extend(['--override', item])\n",
    "    if enable_rocm_metrics:\n",
    "        cmd.append('--enable-rocm-metrics')\n",
    "\n",
    "    complete_env = _prepare_env(env)\n",
    "\n",
    "    print('Launching torchrun job: ' + ' '.join(shlex.quote(token) for token in cmd))\n",
    "    return subprocess.run(cmd, env=complete_env, check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911cebc",
   "metadata": {},
   "source": [
    "## Example: Quick Smoke Test (Single GPU)\n",
    "\n",
    "Uncomment or adjust the cell below to execute a short (5-step) run directly within the notebook. Increase `training.max_steps` or remove the overrides for longer profiling sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ed955d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching single-process training: /opt/conda/envs/py_3.10/bin/python ../train.py --config /manrao/jsr_perf_max/aorta/config/default.yaml --override training.log_interval=1 --override training.max_steps=5 --override training.output_dir=notebook_artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 22:57:58,424 | INFO | aorta.training.fsdp_trainer | Initialised distributed training | backend=nccl rank=0 world=1 local_rank=0 device=cuda:0\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:430: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
      "  warnings.warn(\n",
      "2025-09-30 22:58:01,189 | INFO | aorta.training.fsdp_trainer | epoch=0 step=0 loss=0.35691 lr=0.000006 overlap=0.552ms compute=1215.788ms\n",
      "2025-09-30 22:58:01,313 | INFO | aorta.training.fsdp_trainer | epoch=0 step=5 loss=0.34644 lr=0.000021 overlap=0.277ms compute=18.715ms\n",
      "2025-09-30 22:58:01,430 | INFO | aorta.training.fsdp_trainer | epoch=0 step=10 loss=0.33926 lr=0.000036 overlap=0.300ms compute=18.395ms\n",
      "2025-09-30 22:58:01,539 | INFO | aorta.training.fsdp_trainer | epoch=0 step=15 loss=0.35623 lr=0.000051 overlap=0.308ms compute=18.479ms\n",
      "2025-09-30 22:58:01,647 | INFO | aorta.training.fsdp_trainer | epoch=0 step=20 loss=0.33731 lr=0.000066 overlap=0.321ms compute=18.776ms\n",
      "2025-09-30 22:58:01,755 | INFO | aorta.training.fsdp_trainer | epoch=0 step=25 loss=0.34871 lr=0.000081 overlap=0.297ms compute=18.563ms\n",
      "2025-09-30 22:58:01,863 | INFO | aorta.training.fsdp_trainer | epoch=0 step=30 loss=0.35680 lr=0.000096 overlap=0.283ms compute=18.502ms\n",
      "2025-09-30 22:58:01,970 | INFO | aorta.training.fsdp_trainer | epoch=0 step=35 loss=0.35816 lr=0.000111 overlap=0.285ms compute=18.399ms\n",
      "2025-09-30 22:58:02,077 | INFO | aorta.training.fsdp_trainer | epoch=0 step=40 loss=0.34578 lr=0.000126 overlap=0.347ms compute=18.380ms\n",
      "2025-09-30 22:58:02,184 | INFO | aorta.training.fsdp_trainer | epoch=0 step=45 loss=0.34667 lr=0.000141 overlap=0.283ms compute=18.673ms\n",
      "2025-09-30 22:58:02,291 | INFO | aorta.training.fsdp_trainer | epoch=0 step=50 loss=0.36778 lr=0.000156 overlap=0.297ms compute=18.322ms\n",
      "2025-09-30 22:58:02,520 | INFO | aorta.training.fsdp_trainer | epoch=0 step=55 loss=0.34171 lr=0.000171 overlap=0.279ms compute=24.460ms\n",
      "2025-09-30 22:58:02,636 | INFO | aorta.training.fsdp_trainer | epoch=0 step=60 loss=0.34949 lr=0.000186 overlap=0.336ms compute=18.528ms\n",
      "2025-09-30 22:58:02,744 | INFO | aorta.training.fsdp_trainer | epoch=0 step=65 loss=0.34330 lr=0.000201 overlap=0.325ms compute=18.592ms\n",
      "2025-09-30 22:58:02,852 | INFO | aorta.training.fsdp_trainer | epoch=0 step=70 loss=0.33640 lr=0.000216 overlap=0.297ms compute=18.410ms\n",
      "2025-09-30 22:58:02,960 | INFO | aorta.training.fsdp_trainer | epoch=0 step=75 loss=0.35416 lr=0.000231 overlap=0.345ms compute=18.434ms\n",
      "2025-09-30 22:58:03,067 | INFO | aorta.training.fsdp_trainer | epoch=0 step=80 loss=0.33410 lr=0.000246 overlap=0.364ms compute=18.572ms\n",
      "2025-09-30 22:58:03,175 | INFO | aorta.training.fsdp_trainer | epoch=0 step=85 loss=0.33517 lr=0.000261 overlap=0.333ms compute=18.621ms\n",
      "2025-09-30 22:58:03,283 | INFO | aorta.training.fsdp_trainer | epoch=0 step=90 loss=0.35475 lr=0.000276 overlap=0.329ms compute=18.561ms\n",
      "2025-09-30 22:58:03,391 | INFO | aorta.training.fsdp_trainer | epoch=0 step=95 loss=0.34950 lr=0.000291 overlap=0.309ms compute=18.612ms\n",
      "2025-09-30 22:58:03,499 | INFO | aorta.training.fsdp_trainer | epoch=0 step=100 loss=0.33619 lr=0.000300 overlap=0.307ms compute=18.780ms\n",
      "2025-09-30 22:58:03,607 | INFO | aorta.training.fsdp_trainer | epoch=0 step=105 loss=0.34031 lr=0.000299 overlap=0.336ms compute=18.477ms\n",
      "2025-09-30 22:58:03,715 | INFO | aorta.training.fsdp_trainer | epoch=0 step=110 loss=0.33501 lr=0.000298 overlap=0.366ms compute=18.473ms\n",
      "2025-09-30 22:58:03,823 | INFO | aorta.training.fsdp_trainer | epoch=0 step=115 loss=0.33614 lr=0.000297 overlap=0.284ms compute=18.592ms\n",
      "2025-09-30 22:58:03,931 | INFO | aorta.training.fsdp_trainer | epoch=0 step=120 loss=0.34598 lr=0.000297 overlap=0.331ms compute=18.755ms\n",
      "2025-09-30 22:58:04,038 | INFO | aorta.training.fsdp_trainer | epoch=0 step=125 loss=0.35496 lr=0.000296 overlap=0.325ms compute=18.630ms\n",
      "2025-09-30 22:58:04,146 | INFO | aorta.training.fsdp_trainer | epoch=0 step=130 loss=0.34561 lr=0.000295 overlap=0.319ms compute=18.458ms\n",
      "2025-09-30 22:58:04,254 | INFO | aorta.training.fsdp_trainer | epoch=0 step=135 loss=0.36423 lr=0.000294 overlap=0.309ms compute=18.510ms\n",
      "2025-09-30 22:58:04,362 | INFO | aorta.training.fsdp_trainer | epoch=0 step=140 loss=0.35045 lr=0.000294 overlap=0.306ms compute=18.306ms\n",
      "2025-09-30 22:58:04,469 | INFO | aorta.training.fsdp_trainer | epoch=0 step=145 loss=0.33935 lr=0.000293 overlap=0.281ms compute=18.458ms\n",
      "2025-09-30 22:58:04,576 | INFO | aorta.training.fsdp_trainer | epoch=0 step=150 loss=0.34403 lr=0.000292 overlap=0.325ms compute=18.527ms\n",
      "2025-09-30 22:58:04,684 | INFO | aorta.training.fsdp_trainer | epoch=0 step=155 loss=0.35813 lr=0.000291 overlap=0.285ms compute=18.528ms\n",
      "2025-09-30 22:58:04,791 | INFO | aorta.training.fsdp_trainer | epoch=0 step=160 loss=0.34667 lr=0.000290 overlap=0.295ms compute=18.573ms\n",
      "2025-09-30 22:58:04,899 | INFO | aorta.training.fsdp_trainer | epoch=0 step=165 loss=0.35360 lr=0.000290 overlap=0.311ms compute=18.269ms\n",
      "2025-09-30 22:58:05,006 | INFO | aorta.training.fsdp_trainer | epoch=0 step=170 loss=0.35129 lr=0.000289 overlap=0.292ms compute=18.292ms\n",
      "2025-09-30 22:58:05,114 | INFO | aorta.training.fsdp_trainer | epoch=0 step=175 loss=0.36560 lr=0.000288 overlap=0.309ms compute=18.350ms\n",
      "2025-09-30 22:58:05,236 | INFO | aorta.training.fsdp_trainer | epoch=0 step=180 loss=0.34291 lr=0.000287 overlap=0.330ms compute=18.279ms\n",
      "2025-09-30 22:58:05,344 | INFO | aorta.training.fsdp_trainer | epoch=0 step=185 loss=0.35900 lr=0.000286 overlap=0.285ms compute=18.451ms\n",
      "2025-09-30 22:58:05,452 | INFO | aorta.training.fsdp_trainer | epoch=0 step=190 loss=0.33468 lr=0.000286 overlap=0.289ms compute=18.357ms\n",
      "2025-09-30 22:58:05,560 | INFO | aorta.training.fsdp_trainer | epoch=0 step=195 loss=0.34016 lr=0.000285 overlap=0.290ms compute=18.394ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process exited with return code 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example overrides tailored for a fast validation run.\n",
    "quick_overrides = {\n",
    "    \"training.max_steps\": 5,\n",
    "    \"training.log_interval\": 1,\n",
    "    \"training.output_dir\": \"notebook_artifacts\",\n",
    "}\n",
    "\n",
    "# Set `run = True` when you're ready to launch.\n",
    "run = True\n",
    "\n",
    "if run:\n",
    "    result = run_training_single_process(\n",
    "        overrides=quick_overrides,\n",
    "        enable_rocm_metrics=False,\n",
    "    )\n",
    "    print(f\"Process exited with return code {result.returncode}\")\n",
    "else:\n",
    "    print(\"Set `run = True` to kick off the sample training run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602d788",
   "metadata": {},
   "source": [
    "## Example: Multi-GPU Launch with torchrun\n",
    "\n",
    "Use this helper when you want the notebook to orchestrate a full distributed job across all visible GPUs. Ensure that the notebook kernel is running on the head node with access to the target devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca194844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set `launch_multi_gpu = True` to start the distributed run.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example torchrun invocation (edit `num_processes` to match your GPU count).\n",
    "# To enable ROCm metrics collection add `enable_rocm_metrics=True`.\n",
    "\n",
    "multi_gpu_overrides = {\n",
    "    \"training.max_steps\": 50,\n",
    "    \"training.output_dir\": \"notebook_artifacts_multi\",\n",
    "}\n",
    "\n",
    "# Change to True when you want to launch.\n",
    "launch_multi_gpu = False\n",
    "\n",
    "if launch_multi_gpu:\n",
    "    if torch is None:\n",
    "        raise RuntimeError(\"PyTorch is required to discover device count before launching torchrun.\")\n",
    "    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    result = run_training_torchrun(\n",
    "        num_processes=gpu_count,\n",
    "        overrides=multi_gpu_overrides,\n",
    "        extra_torchrun_args=(\"--rdzv_backend\", \"c10d\"),\n",
    "    )\n",
    "    print(f\"torchrun exited with return code {result.returncode}\")\n",
    "else:\n",
    "    print(\"Set `launch_multi_gpu = True` to start the distributed run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Inspect the generated artefacts (logs, JSONL timelines, traces) inside the configured `training.output_dir`.\n",
    "- Use `analysis/overlap_report.py` from the notebook (for example `!python analysis/overlap_report.py ...`) to compare ROCm vs CUDA runs once you have both datasets.\n",
    "- Update the overrides to toggle profiling features (`profiling.enabled`, `profiling.chrome_trace`, etc.) or to adjust model scale for stress testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618fc32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
